{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Differential privacy image_captioning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2s1A9eLRPEj"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8l4RJ0XRPEm"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# You'll generate plots of attention in order to see which parts of an image\n",
        "# our model focuses on during captioning\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn includes many helpful utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krQuPYTtRPE7",
        "outputId": "d9afa2c0-f84c-4eb2-9220-2ec4d643c344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Download caption annotation files\n",
        "annotation_folder = '/annotations/'\n",
        "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
        "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
        "                                          cache_subdir=os.path.abspath('.'),\n",
        "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
        "                                          extract = True)\n",
        "  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
        "  os.remove(annotation_zip)\n",
        "\n",
        "# Download image files\n",
        "image_folder = '/train2014/'\n",
        "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
        "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
        "                                      extract = True)\n",
        "  PATH = os.path.dirname(image_zip) + image_folder\n",
        "  os.remove(image_zip)\n",
        "else:\n",
        "  PATH = os.path.abspath('.') + image_folder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "252878848/252872794 [==============================] - 3s 0us/step\n",
            "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
            "13510574080/13510573713 [==============================] - 326s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uie8mkvO7MW3"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G3b8x8_RPFD"
      },
      "source": [
        "# Read the json file\n",
        "with open(annotation_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Store captions and image names in vectors\n",
        "all_captions = []\n",
        "all_img_name_vector = []\n",
        "\n",
        "for annot in annotations['annotations']:\n",
        "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
        "    image_id = annot['image_id']\n",
        "    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
        "\n",
        "    all_img_name_vector.append(full_coco_image_path)\n",
        "    all_captions.append(caption)\n",
        "\n",
        "# Shuffle captions and image_names together\n",
        "# Set a random state\n",
        "train_captions, img_name_vector = shuffle(all_captions,\n",
        "                                          all_img_name_vector,\n",
        "                                          random_state=1)\n",
        "\n",
        "# Select the first 30000 captions from the shuffled set\n",
        "num_examples = 30000\n",
        "train_captions = train_captions[:num_examples]\n",
        "img_name_vector = img_name_vector[:num_examples]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPBMgK34RPFL",
        "outputId": "8effd1bf-41c4-4b1d-cab5-68faabde0985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_captions), len(all_captions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 414113)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXR0217aRPFR"
      },
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD3vW4SsRPFW",
        "outputId": "e3cce9af-7d6a-4e41-9adb-73b2c0d308ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIJDL-fqnZoG"
      },
      "source": [
        "#!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1Z6s2EPnfNU"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx_fvbVgRPGQ",
        "outputId": "80ddb5e4-fedc-4644-854f-fad3986624ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Get unique images\n",
        "encode_train = sorted(set(img_name_vector))\n",
        "\n",
        "# Feel free to change batch_size according to your system configuration\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
        "\n",
        "for img, path in tqdm(image_dataset):\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1622it [07:16,  3.71it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZfK8RhQRPFj"
      },
      "source": [
        "# Find the maximum length of any caption in our dataset\n",
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJGE34aiRPFo"
      },
      "source": [
        "# Choose the top 5000 words from the vocabulary\n",
        "top_k = 5000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q44tNQVRPFt"
      },
      "source": [
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fpJb5ojRPFv"
      },
      "source": [
        "# Create the tokenized vectors\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AidglIZVRPF4"
      },
      "source": [
        "# Pad each vector to the max_length of the captions\n",
        "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL0wkttkRPGA"
      },
      "source": [
        "# Calculates the max_length, which is used to store the attention weights\n",
        "max_length = calc_max_length(train_seqs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS7DDMszRPGF"
      },
      "source": [
        "\n",
        "# Create training and validation sets using an 80-20 split\n",
        "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
        "                                                                    cap_vector,\n",
        "                                                                    test_size=0.2,\n",
        "                                                                    random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmViPkRFRPGH",
        "outputId": "71db918f-c55c-445b-f7fc-3ba69afd49a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 24000, 6000, 6000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3TnZ1ToRPGV"
      },
      "source": [
        "# Feel free to change these parameters according to your system's configuration\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = top_k + 1\n",
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmZS2N0bXG3T"
      },
      "source": [
        "# Load the numpy files\n",
        "def map_func(img_name, cap):\n",
        "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "  return img_tensor, cap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDF_Nm3tRPGZ"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se17H5HyqybU",
        "outputId": "78ee1c10-d1b8-4e9e-f524-3dfcdb08cd0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: (<unknown>, <unknown>), types: (tf.float32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UleKGdaQqyFO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja2LFTMSdeV3"
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # score shape == (batch_size, 64, hidden_size)\n",
        "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "\n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    # you get 1 at the last axis because you are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ7R1RxHRPGf"
      },
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it using pickle\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9UbGQmERPGi"
      },
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs_Sr03wRPGk"
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vAPjIjBpfQG",
        "outputId": "e2915014-5162-423a-e40c-8930ad2eb6cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install tensorflow-privacy\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-privacy in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.6/dist-packages (from tensorflow-privacy) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from tensorflow-privacy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=0.17->tensorflow-privacy) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdN403wkpfY6"
      },
      "source": [
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
        "l2_norm_clip = 1.5\n",
        "noise_multiplier = 1.3\n",
        "num_microbatches = 64\n",
        "learning_rate = 0.006\n",
        "batch_size=64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctISFwUIpuZO"
      },
      "source": [
        "optimizer = DPGradientDescentGaussianOptimizer(\n",
        "    l2_norm_clip=l2_norm_clip,\n",
        "    noise_multiplier=noise_multiplier,\n",
        "    num_microbatches=num_microbatches,\n",
        "    learning_rate=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3Mkm_Gcpud9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3h1cQnRpuop"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzBOz2FHpukl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wCvlRIHpuhX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bYN7xA0RPGl"
      },
      "source": [
        "# optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  #loss_ *= mask\n",
        "\n",
        "  return loss_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A3Ni64joyab"
      },
      "source": [
        "## Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpJAqPMWo0uE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUkbqhc_uObw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1QIXzwdwRLd"
      },
      "source": [
        "from absl import logging\n",
        "import collections\n",
        "\n",
        "from tensorflow_privacy.privacy.analysis import privacy_ledger\n",
        "from tensorflow_privacy.privacy.dp_query import gaussian_query\n",
        "\n",
        "def make_optimizer_class(cls):\n",
        "  \"\"\"Constructs a DP optimizer class from an existing one.\"\"\"\n",
        "  parent_code = tf.compat.v1.train.Optimizer.compute_gradients.__code__\n",
        "  child_code = cls.compute_gradients.__code__\n",
        "  GATE_OP = tf.compat.v1.train.Optimizer.GATE_OP  # pylint: disable=invalid-name\n",
        "  if child_code is not parent_code:\n",
        "    logging.warning(\n",
        "        'WARNING: Calling make_optimizer_class() on class %s that overrides '\n",
        "        'method compute_gradients(). Check to ensure that '\n",
        "        'make_optimizer_class() does not interfere with overridden version.',\n",
        "        cls.__name__)\n",
        "\n",
        "  class DPOptimizerClass(cls):\n",
        "    \"\"\"Differentially private subclass of given class cls.\"\"\"\n",
        "\n",
        "    _GlobalState = collections.namedtuple(\n",
        "      '_GlobalState', ['l2_norm_clip', 'stddev'])\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        dp_sum_query,\n",
        "        num_microbatches=None,\n",
        "        unroll_microbatches=False,\n",
        "        *args,  # pylint: disable=keyword-arg-before-vararg, g-doc-args\n",
        "        **kwargs):\n",
        "      \"\"\"Initialize the DPOptimizerClass.\n",
        "\n",
        "      Args:\n",
        "        dp_sum_query: DPQuery object, specifying differential privacy\n",
        "          mechanism to use.\n",
        "        num_microbatches: How many microbatches into which the minibatch is\n",
        "          split. If None, will default to the size of the minibatch, and\n",
        "          per-example gradients will be computed.\n",
        "        unroll_microbatches: If true, processes microbatches within a Python\n",
        "          loop instead of a tf.while_loop. Can be used if using a tf.while_loop\n",
        "          raises an exception.\n",
        "      \"\"\"\n",
        "      super(DPOptimizerClass, self).__init__(*args, **kwargs)\n",
        "      self._dp_sum_query = dp_sum_query\n",
        "      self._num_microbatches = num_microbatches\n",
        "      self._global_state = self._dp_sum_query.initial_global_state()\n",
        "      # TODO(b/122613513): Set unroll_microbatches=True to avoid this bug.\n",
        "      # Beware: When num_microbatches is large (>100), enabling this parameter\n",
        "      # may cause an OOM error.\n",
        "      self._unroll_microbatches = unroll_microbatches\n",
        "\n",
        "    def compute_gradients(self,\n",
        "                          loss,\n",
        "                          var_list,\n",
        "                          gate_gradients=GATE_OP,\n",
        "                          aggregation_method=None,\n",
        "                          colocate_gradients_with_ops=False,\n",
        "                          grad_loss=None,\n",
        "                          gradient_tape=None,\n",
        "                          curr_noise_mult=0,\n",
        "                          curr_norm_clip=1):\n",
        "\n",
        "      self._dp_sum_query = gaussian_query.GaussianSumQuery(curr_norm_clip, \n",
        "                                                           curr_norm_clip*curr_noise_mult)\n",
        "      self._global_state = self._dp_sum_query.make_global_state(curr_norm_clip, \n",
        "                                                                curr_norm_clip*curr_noise_mult)\n",
        "      \n",
        "\n",
        "      # TF is running in Eager mode, check we received a vanilla tape.\n",
        "      if not gradient_tape:\n",
        "        raise ValueError('When in Eager mode, a tape needs to be passed.')\n",
        "\n",
        "      vector_loss = loss\n",
        "      if self._num_microbatches is None:\n",
        "        self._num_microbatches = tf.shape(input=vector_loss)[0]\n",
        "      sample_state = self._dp_sum_query.initial_sample_state(var_list)\n",
        "      microbatches_losses = tf.reshape(vector_loss, [self._num_microbatches, -1])\n",
        "      sample_params = (self._dp_sum_query.derive_sample_params(self._global_state))\n",
        "\n",
        "      def process_microbatch(i, sample_state):\n",
        "        \"\"\"Process one microbatch (record) with privacy helper.\"\"\"\n",
        "        microbatch_loss = tf.reduce_mean(input_tensor=tf.gather(microbatches_losses, [i]))\n",
        "        grads = gradient_tape.gradient(microbatch_loss, var_list)\n",
        "        sample_state = self._dp_sum_query.accumulate_record(sample_params, sample_state, grads)\n",
        "        return sample_state\n",
        "    \n",
        "      for idx in range(self._num_microbatches):\n",
        "        sample_state = process_microbatch(idx, sample_state)\n",
        "\n",
        "      if curr_noise_mult > 0:\n",
        "        grad_sums, self._global_state = (self._dp_sum_query.get_noised_result(sample_state, self._global_state))\n",
        "      else:\n",
        "        grad_sums = sample_state\n",
        "\n",
        "      def normalize(v):\n",
        "        return v / tf.cast(self._num_microbatches, tf.float32)\n",
        "\n",
        "      final_grads = tf.nest.map_structure(normalize, grad_sums)\n",
        "      grads_and_vars = final_grads#list(zip(final_grads, var_list))\n",
        "    \n",
        "      return grads_and_vars\n",
        "\n",
        "  return DPOptimizerClass\n",
        "\n",
        "\n",
        "def make_gaussian_optimizer_class(cls):\n",
        "  \"\"\"Constructs a DP optimizer with Gaussian averaging of updates.\"\"\"\n",
        "\n",
        "  class DPGaussianOptimizerClass(make_optimizer_class(cls)):\n",
        "    \"\"\"DP subclass of given class cls using Gaussian averaging.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        l2_norm_clip,\n",
        "        noise_multiplier,\n",
        "        num_microbatches=None,\n",
        "        ledger=None,\n",
        "        unroll_microbatches=False\n",
        "        *args,  # pylint: disable=keyword-arg-before-vararg\n",
        "        **kwargs):\n",
        "      dp_sum_query = gaussian_query.GaussianSumQuery(\n",
        "          l2_norm_clip, l2_norm_clip * noise_multiplier)\n",
        "\n",
        "      if ledger:\n",
        "        dp_sum_query = privacy_ledger.QueryWithLedger(dp_sum_query,\n",
        "                                                      ledger=ledger)\n",
        "\n",
        "      super(DPGaussianOptimizerClass, self).__init__(\n",
        "          dp_sum_query,\n",
        "          num_microbatches,\n",
        "          unroll_microbatches,\n",
        "          *args,\n",
        "          **kwargs)\n",
        "\n",
        "    @property\n",
        "    def ledger(self):\n",
        "      return self._dp_sum_query.ledger\n",
        "\n",
        "  return DPGaussianOptimizerClass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ein1VGb3wTqj"
      },
      "source": [
        "# AdamOptimizer = tf.compat.v1.train.AdamOptimizer\n",
        "# DPAdamOptimizer_NEW = make_gaussian_optimizer_class(AdamOptimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-uI4YD8wT2C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxzmdfC3wTwf"
      },
      "source": [
        "# LR_DISC = tf.compat.v1.train.polynomial_decay(learning_rate=0.150,\n",
        "#                                               global_step=tf.compat.v1.train.get_or_create_global_step(),\n",
        "#                                               decay_steps=10000,\n",
        "#                                               end_learning_rate=0.052,\n",
        "#                                               power=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-RYe5JhOGHl"
      },
      "source": [
        "# LR_DISC = tf.convert_to_tensor(LR_DISC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TIWqn2zwuqt"
      },
      "source": [
        "# optimizer = DPAdamOptimizer_NEW(\n",
        "#    learning_rate = 0.006,\n",
        "#    l2_norm_clip = 1.5,\n",
        "#    noise_multiplier = 1.3,\n",
        "#    num_microbatches = 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoD1Tg-_wu6Y"
      },
      "source": [
        "cross_entropy_DISC = tf.keras.losses.BinaryCrossentropy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4BhXR_Uwu0l"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "figTsKQ0fjYu"
      },
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "  # restoring the latest checkpoint in checkpoint_path\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt4WZ5mhJE-E"
      },
      "source": [
        "# adding this in a separate cell because if you run the training cell\n",
        "# many times, the loss_plot array will be reset\n",
        "loss_plot = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqgyz2ANKlpU"
      },
      "source": [
        "# @tf.function\n",
        "# def train_step(img_tensor, target):\n",
        "#   loss = 0\n",
        "\n",
        "#   # initializing the hidden state for each batch\n",
        "#   # because the captions are not related from image to image\n",
        "#   hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "#   dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "#   with tf.GradientTape() as tape:\n",
        "#       features = encoder(img_tensor)\n",
        "\n",
        "#       for i in range(1, target.shape[1]):\n",
        "#           # passing the features through the decoder\n",
        "#           predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "#           loss += tf.reduce_mean(loss_function(target[:, i], predictions))\n",
        "\n",
        "#           # using teacher forcing\n",
        "#           dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "#   total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "#   trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "#   # gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "#   # optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "#   r=optimizer.compute_gradients(loss_function(p[:, i], predictions),trainable_variables)\n",
        "#   optimizer.apply_gradients(zip(r, trainable_variables))\n",
        "\n",
        "#   return loss, total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT_zA0bRKMsY"
      },
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "  loss = 0\n",
        "\n",
        "  # initializing the hidden state for each batch\n",
        "  # because the captions are not related from image to image\n",
        "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "      features = encoder(img_tensor)\n",
        "\n",
        "      for i in range(1, target.shape[1]):\n",
        "          # passing the features through the decoder\n",
        "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "          loss += tf.reduce_mean(loss_function(target[:, i], predictions))\n",
        "\n",
        "          # using teacher forcing\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "      #r=optimizer.compute_gradients(loss_function(p[:, i], predictions),trainable_variables,gradient_tape=tape)\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "  \n",
        "  #optimizer.apply_gradients(zip(r, trainable_variables))\n",
        "\n",
        "  return loss, total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymjFrNMb5ysa"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlA4VIQpRPGo",
        "outputId": "df9097d0-8f4b-4f8c-a835-a083eb67f48d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      ckpt_manager.save()\n",
        "\n",
        "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
        "                                         total_loss/num_steps))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 8.3789\n",
            "Epoch 1 Batch 100 Loss 2.0801\n",
            "Epoch 1 Batch 200 Loss 2.1271\n",
            "Epoch 1 Batch 300 Loss 1.9731\n",
            "Epoch 1 Loss 2.388613\n",
            "Time taken for 1 epoch 388.2658030986786 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.9821\n",
            "Epoch 2 Batch 100 Loss 1.9302\n",
            "Epoch 2 Batch 200 Loss 2.0383\n",
            "Epoch 2 Batch 300 Loss 1.4490\n",
            "Epoch 2 Loss 1.805375\n",
            "Time taken for 1 epoch 347.6347737312317 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.2428\n",
            "Epoch 3 Batch 100 Loss 1.2528\n",
            "Epoch 3 Batch 200 Loss 1.2992\n",
            "Epoch 3 Batch 300 Loss 1.1242\n",
            "Epoch 3 Loss 1.260809\n",
            "Time taken for 1 epoch 345.62525391578674 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.2072\n",
            "Epoch 4 Batch 100 Loss 1.1893\n",
            "Epoch 4 Batch 200 Loss 1.1680\n",
            "Epoch 4 Batch 300 Loss 1.2578\n",
            "Epoch 4 Loss 1.198928\n",
            "Time taken for 1 epoch 344.56497979164124 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.2076\n",
            "Epoch 5 Batch 100 Loss 1.1864\n",
            "Epoch 5 Batch 200 Loss 1.1906\n",
            "Epoch 5 Batch 300 Loss 1.1930\n",
            "Epoch 5 Loss 1.154372\n",
            "Time taken for 1 epoch 344.7721402645111 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.0841\n",
            "Epoch 6 Batch 100 Loss 1.1493\n",
            "Epoch 6 Batch 200 Loss 1.1136\n",
            "Epoch 6 Batch 300 Loss 1.1593\n",
            "Epoch 6 Loss 1.119122\n",
            "Time taken for 1 epoch 343.90716552734375 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.0731\n",
            "Epoch 7 Batch 100 Loss 1.1438\n",
            "Epoch 7 Batch 200 Loss 1.0989\n",
            "Epoch 7 Batch 300 Loss 1.1026\n",
            "Epoch 7 Loss 1.095946\n",
            "Time taken for 1 epoch 343.49916195869446 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.1123\n",
            "Epoch 8 Batch 100 Loss 1.0761\n",
            "Epoch 8 Batch 200 Loss 1.0859\n",
            "Epoch 8 Batch 300 Loss 1.1382\n",
            "Epoch 8 Loss 1.074717\n",
            "Time taken for 1 epoch 343.3023042678833 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.1148\n",
            "Epoch 9 Batch 100 Loss 1.0302\n",
            "Epoch 9 Batch 200 Loss 1.0665\n",
            "Epoch 9 Batch 300 Loss 1.0463\n",
            "Epoch 9 Loss 1.049975\n",
            "Time taken for 1 epoch 342.14160656929016 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.0074\n",
            "Epoch 10 Batch 100 Loss 1.1376\n",
            "Epoch 10 Batch 200 Loss 1.0068\n",
            "Epoch 10 Batch 300 Loss 0.9905\n",
            "Epoch 10 Loss 1.024868\n",
            "Time taken for 1 epoch 342.7623641490936 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.9826\n",
            "Epoch 11 Batch 100 Loss 0.9142\n",
            "Epoch 11 Batch 200 Loss 1.0109\n",
            "Epoch 11 Batch 300 Loss 0.9164\n",
            "Epoch 11 Loss 1.002653\n",
            "Time taken for 1 epoch 343.1390731334686 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.9454\n",
            "Epoch 12 Batch 100 Loss 0.9945\n",
            "Epoch 12 Batch 200 Loss 0.9800\n",
            "Epoch 12 Batch 300 Loss 0.9285\n",
            "Epoch 12 Loss 0.982393\n",
            "Time taken for 1 epoch 342.52636671066284 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.9630\n",
            "Epoch 13 Batch 100 Loss 0.9979\n",
            "Epoch 13 Batch 200 Loss 0.9343\n",
            "Epoch 13 Batch 300 Loss 0.9942\n",
            "Epoch 13 Loss 0.964911\n",
            "Time taken for 1 epoch 342.4298791885376 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.8854\n",
            "Epoch 14 Batch 100 Loss 1.0156\n",
            "Epoch 14 Batch 200 Loss 0.9437\n",
            "Epoch 14 Batch 300 Loss 0.8949\n",
            "Epoch 14 Loss 0.949380\n",
            "Time taken for 1 epoch 343.75534319877625 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.9014\n",
            "Epoch 15 Batch 100 Loss 0.9113\n",
            "Epoch 15 Batch 200 Loss 0.9799\n",
            "Epoch 15 Batch 300 Loss 0.9076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wm83G-ZBPcC"
      },
      "source": [
        "plt.plot(loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGvOcLQKghXN"
      },
      "source": [
        "## Caption!\n",
        "\n",
        "* The evaluate function is similar to the training loop, except you don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the end token.\n",
        "* And store the attention weights for every time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCWpDtyNRPGs"
      },
      "source": [
        "def evaluate(image):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD_y7PD6RPGt"
      },
      "source": [
        "def plot_attention(image, result, attention_plot):\n",
        "    temp_image = np.array(Image.open(image))\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for l in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
        "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
        "        ax.set_title(result[l])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdAd_eMsD-13"
      },
      "source": [
        "# def predict(Image_path):\n",
        "    \n",
        "#     checkpoint_path = \"/content/drive/My Drive/checkpoint/train/ckpt-4\"\n",
        "#     #train_captions = load(open('./drive/My Drive/checkpoint/captions.pkl', 'rb'))\n",
        "\n",
        "\n",
        "#     # Choose the top 5000 words from the vocabulary\n",
        "#     #top_k = 5000   \n",
        "#     #train_seqs , tokenizer = tokenize_caption(top_k ,train_captions)\n",
        "\n",
        "#     #restoring the model\n",
        "    \n",
        "#     ckpt = tf.train.Checkpoint(encoder=encoder,decoder=decoder,optimizer = optimizer)\n",
        "#     ckpt.restore(checkpoint_path)\n",
        "\n",
        "#     new_img =  Image_path\n",
        "#     #image_extension = Image_path[-3:]\n",
        "\n",
        "#     #image_path = tf.keras.utils.get_file('image'+image_extension,origin=Image_path)\n",
        "\n",
        "#     result, attention_plot = evaluate(new_img)\n",
        "#     for i in result:\n",
        "#         if i==\"<unk>\":\n",
        "#             result.remove(i)\n",
        "#         else:\n",
        "#             pass\n",
        "    \n",
        "#     print('Prediction Caption: ', ' '.join(result).rsplit(' ', 1)[0])\n",
        "\n",
        "#     plot_attention(new_img, result, attention_plot)\n",
        "    \n",
        "#     #image_plot(new_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x8RiPHe_4qI"
      },
      "source": [
        "# captions on the validation set\n",
        "rid = np.random.randint(0, len(img_name_val))\n",
        "image = img_name_val[rid]\n",
        "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "result, attention_plot = evaluate(image)\n",
        "\n",
        "print ('Real Caption:', real_caption)\n",
        "print ('Prediction Caption:', ' '.join(result))\n",
        "plt.show(image)\n",
        "plot_attention(image, result, attention_plot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rprk3HEvZuxb"
      },
      "source": [
        "## Try it on your own images\n",
        "For fun, below we've provided a method you can use to caption your own images with the model we've just trained. Keep in mind, it was trained on a relatively small amount of data, and your images may be different from the training data (so be prepared for weird results!)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Psd1quzaAWg"
      },
      "source": [
        "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "image_extension = image_url[-4:]\n",
        "image_path = tf.keras.utils.get_file('image'+image_extension,\n",
        "                                     origin=image_url)\n",
        "\n",
        "result, attention_plot = evaluate(image_path)\n",
        "print ('Prediction Caption:', ' '.join(result))\n",
        "plot_attention(image_path, result, attention_plot)\n",
        "# opening the image\n",
        "Image.open(image_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJZXyJco6uLO"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "Congrats! You've just trained an image captioning model with attention. Next, take a look at this example [Neural Machine Translation with Attention](../sequences/nmt_with_attention.ipynb). It uses a similar architecture to translate between Spanish and English sentences. You can also experiment with training the code in this notebook on a different dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkEWnAV5sE2w"
      },
      "source": [
        "# !cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp-r8mkWsLGY"
      },
      "source": [
        "# !ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jes5NWZWsUoM"
      },
      "source": [
        "# !cd checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuupyhgHseTM"
      },
      "source": [
        "# for dir in os.listdir('/content/checkpoints/train'):\n",
        "#   print(dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "monbhTXhsiAV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_TGBhXevm82"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9rWTCG3voYj"
      },
      "source": [
        "# checkpoint_path = '/content/checkpoints/train'\n",
        "# drive_path = '/content/drive/My Drive/Checkpoints'\n",
        "\n",
        "# !cp -r '/content/checkpoints/train' '/content/drive/My Drive/Checkpoints'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSxtda52E2ll"
      },
      "source": [
        "# image_url = 'https://c8.alamy.com/comp/RD5HX6/young-boy-flying-a-kite-RD5HX6.jpg'\n",
        "# image_extension = image_url[-4:]\n",
        "# image_path = tf.keras.utils.get_file('image'+image_extension,\n",
        "#                                      origin=image_url)\n",
        "\n",
        "# predict(image_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OweVh5pmGxU_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}